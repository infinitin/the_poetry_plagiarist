\chapter{Evaluation}
\ifpdf
    \graphicspath{{Evaluation/EvaluationFigs/PNG/}{Evaluation/EvaluationFigs/PDF/}{Evaluation/EvaluationFigs/}}
\else
    \graphicspath{{Evaluation/EvaluationFigs/EPS/}{Evaluation/EvaluationFigs/}}
\fi

We evaluate this program by looking at the results of analysing 450 limericks, 718 haiku and all 154 of Shakespeare's Sonnets. These three types of poetry cover a large range of techniques and are of varying lengths so we can see changes in performance and quality with poem size.

\section{Comparison of Analysis Results to Literary Theory}
A major goal of this project is to show that the structure and content of types of poetry can be derived by analysing corpuses of collections. We would like to investigate how our results matches up with those documented in literary theory, as described by Young Writers[ref], a poetry and creative writing resource for UK schools, and poets.org[ref], supported by the Academy of American Poets.

\subsection{Limericks}

\begin{description}
\item[One stanza, five lines, no repetition]  \hfill \\
All of the poems in our corpus follow this rule, as seen in Figure BLAH.


\item[Usually begins with \textit{'There was a...'} and ends with a name, person or place]  \hfill \\
Figure BLAH shows the N-Grams for line 1. We can see that \textit{'There be a'}, the lemmatised version of \textit{'There was a'} is used with significant frequency. We can also see in Figure BLAH that limericks largely talk about animate objects (such as people) or non-objects (such as places). Furthermore, Figure BLAH shows that the probability of a \textit{Named} or \textit{AtLocation} relation is higher in the first line than any other line.

Overall, our results supports the theory. However, since the analysis omits Named Entity Recognition, we are unable to specifically say whether or not it was a person or a place.

Furthermore, our results only show that x poems have this n-gram, which is only X\% of the corpus. We suspect this discrepancy comes from the popularity of Edward Lear poems, of which most start with \textit{'There was a...'}, but poems by other authors do not generally use this as a rule. This comes down to the contents of the corpus. Since it was a collection of previously unseen poems from various sources across the Internet, we do not know the proportion of Edward Lear poems in the corpus before analysis (as required to avoid biasing the corpus as mentioned in Section BLAH). If the poem had a higher proportion of Edward Lear limericks, the frequency of this property would probably increase. However, since we are analysing limericks in general, our result may be representative.

\item[Last line is farfetched or unusual]  \hfill \\
We have no way of determining this from our analysis or results because the social context of the poem was never considered. As a result this feature will only occur by chance, not intention.

\item[Usually nonsense or humour]  \hfill \\
As above.

\item[\textit{AABBA} rhyme scheme]  \hfill \\
Our results show that this is by far the most probable rhyme scheme, as seen in Figure BLAH. The other possibilities are close variations of \textit{AABBA}, eluding to the limitations of the CMU Pronunciation Dictionary and our method of handling words unknown to it, as described in Section BLAH. The results may yet improve if we were to build a state transducer model to predict the most likely rhythm pattern for words that are not in the dictionary.

Despite those shortcomings the results follow the ZipF law as discussed in Section BLAH, which means that we can treat the top result as unambiguous. Therefore, all limericks generated by the system will aim to have the \textit{AABBA} rhyme scheme.

\item[Lines 1, 2 and 5 have 01001001 meter, while lines 3 and 4 have 01001 meter]  \hfill \\
Figures BLAH through BLAH show the stress patterns of lines 1, 2 and 5.

The most popular rhyme scheme in line 1 is the expected one. However, it does not follow the ZipF law because the second option - the expected pattern with an appended unstressed syllable - is not significantly less probable.

The results for the second line are worse because the expected pattern is only third most probable. The others are variations on the expected pattern - first prefixed with a stressed syllable and then appended with an unstressed syllable. However, given the relative proportions of these three, it is not unlikely that the expected pattern will be selected.

The analysis of the fifth line is the worst. Not only is the expected pattern the third most popular, the graph follows the ZipF law, which means that the first, incorrect pattern (expected pattern prefixed with a stressed syllable) will be treated as unambiguous.

In all cases, the most probable patterns include the expected one or variations thereof. This is likely caused by the limitations of the CMU Pronunciation Dictionary again. We do not suspect it is our methodology because we try to accommodate for as many rhythm possibilities during analysis and we make no claims or filters along the way. 

Since our generation phase tries for a best fit rather than strictly the same pattern, this is not a damaging result in that respect.

Figures BLAH and BLAH show the stress patterns of lines 3 and 4. 

In both of these lines, the expected pattern is the third most popular and the most popular is the expected pattern prefixed with a stressed syllable. The most interesting thing is the relative proportions - both times the 101001 pattern would be treated as unambiguous. The reasoning for this comes down to three possibilities:
\begin{itemize}
\item{The limitations of the CMU Pronunciation Dictionary are greater than expected, but consistent. The syllable count is consistently one too high and happens to lead with the added stressed syllable very often as its mistake.}
\item{Our algorithms for detecting rhythm are flawed.}
\item{The corpus is very biased to this particular mistake.}
\item{Limericks actually have 101001 as the expected rhythm so our sources are wrong and perhaps this was never noticed.}
\end{itemize}

The first two are unlikely because the results for lines 1, 2 and 5 as well as for other poems (see following sections) are generally as expected. If either of those were in fact the problem, we would expect many more unexpected results.

The third is a possibility. The corpus of poems was collected from various sources across the Internet, but some websites provided more than others. It is likely that the same author who consistently made the same mistake was published on the same website that made up a large proportion of the corpus.

The fourth possibility requires further investigation into high quality limericks written by well known authors and more sources for the rhythm of limericks. Poets.org credits Edward Lear's \textit{Book of Nonsense}[ref] as the defining anthology for modern limericks.

The first 10 poems of that very book are shown in Appendix BLAH. If we analyse them by eye, the third line has six syllables 5 times and the fourth line has six syllables 8 times. From this we can assume that the expected rhythm of 01001 is actually not that common.

The Book of Limericks (Wordsword Reference) is a published anthology of over 1,800 limericks. This book was not used to build the corpus so it is unlikely that any of them would have been part of the original analysis. We select the first 10 from this analogy, as seen in Appendix \ref{sec:book}, and perform the same analysis by eye. In this case, the third line has six syllables 8 times and the fourth line has six syllables all 10 times.

This suggests that the standard is in fact to have six syllables, so the expected rhythm of 01001 is not correct. Therefore, we look to other sources of literary theory in case this has been pointed out.

http://www.poetry4kids.com/blog/lessons/how-to-write-a-limerick/ has 01001 and their examples use six syllables.
http://www.gigglepoetry.com/poetryclassdetail.aspx?LessonPlanID=2 claims the fourth line is 001001, but their example is five syllables.
http://www.creative-writing-now.com/how-to-write-a-limerick.html
http://www.poetryteachers.com/poetclass/lessons/limerick.html
http://freespace.virgin.net/merrick.sheldon/limerickrules.htm

Nowhere is it mentioned that the rhythm should be 101001. The five sources above teach the rhythm of limericks, but their teachings vary and the examples they use often do not match up with the theory that they give, but in most cases do match with the 101001 candidate.

We therefore, conclude that more investigation needs to be done to decide what is the \textit{actual} expected rhythm of the third and fourth line or limericks, and we further propose that based on our analysis it should be 101001.

\item[Other Noteworthy Results]  \hfill \\
Figure BLAH shows that limericks are usually written in past tense and occasionally in present tense.

A very large proportion are written in third person, as seen in Figure BLAH.

\end{description}


\subsection{Haiku}

\begin{description}
\item[One stanza, three lines, no repetition]  \hfill \\
All of the poems in our corpus follow this rule, as seen in Figure BLAH.

\item[Topic ranges widely, \textit{nature} is very common]  \hfill \\
Figure BLAH shows that there is indeed a wide range of topics including halogen (light), living thing, natural object, cognition, chemical element, organism, body part, atmospheric phenomenon, weather and natural phenomenon; all of which elude to nature. 

Figure BLAH also shows that it generally talks about non-objects, i.e. nouns that do not cast a shadow. This includes nature and weather to back up the earlier graph.

\item[5-7-5 syllabic rhythm]  \hfill \\
Figures BLAH through BLAH show the syllabic rhythm of each line. It is clear that 5-7-5 is a most popular and is taken as unambiguous.

\item[No rhyme scheme]  \hfill \\
Figure BLAH shows that haiku is most likely to not have any specific rhyme scheme (\textit{ABC}). The candidate rhyme schemes occur with low frequency, so the lack of rhyme scheme is treated as unambiguous and any rhyme only occurs by chance.

\item[No specific meter]  \hfill \\
Figures BLAH through BLAH show the stress patterns for haiku. The graphs give a large number of options, but do match the Zipf law, suggesting that the most probable stress pattern is intentional. Given that poets are taught to write haikus without considering meter, we suspect that this may be a coincidence.

\item[Other Noteworthy Results]  \hfill \\
Conversely to limericks, Figure BLAH shows that limericks are usually written in the present tense and occasionally in past tense.

As with limericks, most haiku are written in third person, as seen in Figure BLAH.

\end{description}

\subsection{Shakespearean Sonnets}

\begin{description}
\item[One stanza, fourteen lines, no repetition]  \hfill \\
All of the poems in our corpus follow this rule, as seen in Figure BLAH.

\item[Topic is usually of thought, emotion and ideas]  \hfill \\
This is evident from Figure BLAH, which shows the topics covered are related to thought, emotion and ideas. In particular, psychological feature, cognition, emotion, feeling, relation, state and communication.

Figure BLAH supports this, showing that often non-objects were discussed more than animate and inanimate objects. Such non-objects include though, emotion and ideas.

Figure BLAH also shows that Shakespeare mentioned words such as \textit{'love'}, \textit{'beauty'}, \textit{'fair'} and \textit{'think'} a significant number of times across his sonnets.

\item[\textit{ABABCDCDEFEFGG} rhyme scheme]  \hfill \\
Figure BLAH shows that the most probable rhyme scheme is as expected. However, because of the Shakesperean vocabulary that is not in the CMU Pronunciation Dictionary and the length of the poem, it often failed to complete analysis for certain poems if there were too many possible permutations (see Section BLAH). 

This exposes a limitation on this system that each extra line of the poem potentially increases the number of rhyme permutations exponentially, risking memory errors. A simple solution to this would be to leave the rhyme schemes in their unzipped form as in Figure BLAH and counting the frequency of possible permutations iteratively.

\item[Iambic pentameter for every line]  \hfill \\
Figures BLAH through BLAH show that each line is unambiguously iambic pentameter (0101010101) \textbf{except} for the 13th line, which is unambiguously iambic pentameter except that the starting syllable is stressed.

This is a very exciting result because it is unlikely to be caused a limitation of the CMU Pronunciation Dictionary or our implementation because this discrepancy is so consistent and specific. This is also the \textit{"perfect"} corpus, containing all 154 of Shakespeare's original sonnets that are definitely the defining poems of Shakesperean sonnets.

Most research says that every line of every Shakespearean Sonnet is iambic pentameter. No research could be found to support the stressing of the first syllable of the 13th line, however, we propose that it is a significant result and not a coincidence.

Firstly, iambic pentameter is not even a possibility in our analysis. Secondly, the thirteenth line of the poem is the start of the \textit{'heroic couplet'}, marking a change in the previous quatrains and signalling the impending end of the sonnet. Therefore, there is a motivation to break the monotony and add the extra stress, acting as the start of a strong conclusion.

If analyse some sonnets by eye for this specific aspect, we can find examples where the first syllable \textbf{cannot} be unstressed, either due to the starting word needing two stressed syllables or use of punctuation (particularly exclamation marks) following the first syllable.

\begin{description}
\item[Sonnet I: ] \textit{Pity the world, or else this glutton be,}
\item[Sonnet XIII: ] \textit{O, none but unthrifts! Dear my love, you know}
\item[Sonnet XIX: ] \textit{Yet, do thy worst, old Time: despite thy wrong,}
\item[Sonnet XXIII: ] \textit{O, learn to read what silent love hath writ:}
\item[Sonnet XXVII: ] \textit{Lo! thus, by day my limbs, by night my mind,}
\item[Sonnet XXXIV: ] \textit{Ah! but those tears are pearl which thy love sheds,}
\item[Sonnet XXXVII: ] \textit{Look, what is best, that best I wish in thee:}
\item[Sonnet XLVII: ] \textit{Or, if they sleep, thy picture in my sight}
\item[Sonnet LII: ] \textit{Blessed are you, whose worthiness gives scope,}
\item[Sonnet LV: ] \textit{So, till the judgment that yourself arise,}
\item[Sonnet LIX: ] \textit{O, sure I am, the wits of former days}
\item[Sonnet LXV: ] \textit{O, none, unless this miracle have might,}
\item[Sonnet LXVII: ] \textit{O! him she stores, to show what wealth she had}
\item[Sonnet XCI: ] \textit{Wretched in this alone, that thou mayst take}
\item[Sonnet XCVII: ] \textit{Or, if they sing, 'tis with so dull a cheer}
\item[Sonnet CV: ] \textit{'Fair, kind, and true,' have often lived alone,}
\item[Sonnet CXXV: ] \textit{Hence, thou suborn'd informer! a true soul}
\item[Sonnet CXLIX: ] \textit{But, love, hate on, for now I know thy mind;}
\end{description}

Of the rest of the sonnets where the meter is more ambiguous, we argue that stressing the first syllable is more natural when spoken out loud.

Given the evidence and motivations explained above, we claim that it was Shakespeare's intention to start the 13th line of his sonnets with a stressed syllable, which means that it is \textbf{not iambic pentameter}, contrary to current literacy theory.

\item[Other Noteworthy Results]  \hfill \\
Most sonnets are written in first or second person as seen in Figure BLAH and almost all are written in the present tense, shown in Figure BLAH.
\end{description}


\subsection{Summary}
Overall, our results show that the system's general interpretation of poetic structure and content matches with literary theory despite a relatively low quality corpus. This shows that there is no need to hard-code any rules of poetry, as was done in previous attempts (Section BLAH).

In fact, we have even found two cases where our program is contrary to literacy theory that may be true findings, not errors. It is exciting to see computerised poetry analysis become involved in evolution and understanding of a literary art such as poetry.

The major shortcomings of the current implementation are:
\begin{itemize}
\item{The omission of any Named Entity Recognition limits not only the general contextual understanding of the poem, but also the specific recognition of names and places.}
\item{Our attempt to patch up the limitations of the CMU Pronunciation Dictionary by finding the closest matching word (see Section BLAH) is not as reliable as expected and should be replaced with a state transducer model implementation.}
\item{The memory and time complexity rises exponentially with the length of the poem with the current implementation. As seen with the sonnets compared to the haiku and limericks, the quality of analysis falls as the poems get longer.}
\item{The sentiment of the poem has not been considered at all in this project. However, it is important to note, for example, that limericks are humorous.}
\item{Using our own algorithms for anaphora resolution and presupposition projection, despite it being a prelude to the new proposed method in Section BLAH, is not affective at the moment, so we end up finding more characters than expected.}
\end{itemize}

Content understanding looks promising but still needs lots of work. There is currently no NE Recognition, a rather big omission but easily fixed.

Large number of unknown words (can we get a number?). This makes it extremely slow because we find the closest word by spelling. As we know this does not always work for English, so this can be improved by using the state transducer model. Will also significantly speed up processing because we are not wasting time looking up the closest matching word, which is probably the slowest thing and why non-shakes ones go a lot faster.

Anaphora stuff:
This project does not delve deeply enough into this approach to fully test and evaluate it as it is outside the scope of this project. However, it is a new approach with potential that may be worth investigating further. 



\section{Turing-style Tests}
Can they fool a human?
Is it something that people can use?
Limericks: Blah
Sonnets: Blah
Haiku: Blah


\section{Quality of English}
Our approach to generation is \textbf{context first}, which requires a large vocabulary and control over grammar.
\subsection{Vocabulary and Word Selection}
The vocabulary of the system is dispersed among the third party and internal sources used.
WordNet: 155,000
CMUPD: 133,000
FrameNet: Blah
Knowledge Network: Blah
SimpleNLG Lexicon: Blah

Only x words are shared among all of these sources. Altogether, the number of unique words is y. The Oxford Enlgish Dictionary has z words so there is still much room for improvement. We have no scientific or colloquial vocabulary. 

The average vocabulary of native English speakers is 20,000-35,000[http://www.economist.com/blogs/johnson/2013/05/vocabulary-size], so in that sense we are very good. However, the amount of the vocabulary that is actually used is somewhat limited by the fact that we translate a relation (of which there are relatively few) to phrases. Selection from FrameNet is somewhat limited and not hugely dependable for many words. We use the general takes and receives action relations, but quality will significantly improve if we get more relations like desires, usedfor etc.

\subsection{Grammar}
This means that the quality of English should be rather high. We can check this by running the output through an over-the-counter grammar checker. We find that some percentage of lines do not fail the grammar checker, assuming a full stop at the end of every line.

FrameNet does not have all the answers.

Could take advantage of knowledge of length of syllables to determine the grammatical structure (e.g. intransitive verbs for short sentences).

Enjambent fails.

\section{Exhibition of Poetic Features}
Probabilistic. Rhyme and syllabic rythm are followed most of the time. However, by nature of the results being somewhat ambiguous, the results vary naturally.

Accentual rhythm is a best fit and is not as important as content. If we analyse the poem back using the analysis stage, we find that everything except this matches up with some permutation of the probability distribution, but not necessarily one specific instance. This is good because it means that we are getting \textit{close} to other previous poems, but not directly copying the grammatical structure like in that constraint programming one.

\section{Creativity}
Are the poems creative?
\subsection{FACE and IDEA Descriptive Models}
\subsubsection{FACE Descriptive Model}
\label{sec:face}
A full FACE model has four symptoms: examples, concepts, aesthetics and framing information.

\begin{itemize}
\setlength{\itemsep}{0pt}
\item{\emph{Examples} will be showcased by the templates generated by the Analysis and Abstraction phase.}
\item{\emph{Concepts} are of the form of the algorithm described for the Generation phase as it takes input from the user or online, the results from the Abstraction phase and several third party libraries to output a poem.}
\item{\emph{Aesthetics} are assessed by running the Analysis phase over the poem again. In fact, this happens several times during the creation of the poem. Any faults are reported back to the next iteration of that poem.}
\item{\emph{Framing Information} is the poem created.}
\end{itemize}

Therefore, we can see that this system should fully abide by the FACE Descriptive Model. We will evaluate the results mathematically as per Colton, Charnley and Pease.

\subsubsection{IDEA Descriptive Model}
\label{sec:idea}
A full IDEA model has six stages to which the software can reach. We want our software to be in the, fourth or \emph{Discovery stage}.

\begin{itemize}
\setlength{\itemsep}{0pt}
\item{\emph{Developmental stage}: this system has a full Abstraction phase to avoid the case that all creative acts undertaken by this system are purely based on inspiring examples. So this system will have surpassed this stage.}
\item{\emph{Fine-tuning stage}: the Abstraction phase only looks for a limited number of overlapping features to provide the template, leading to higher level abstraction. For example, it does not use part-of-speech tags from previous examples or any low level abstractions. We believe the system should be able to surpass this level.}
\item{\emph{Re-invention stage}: the system is able to work off a template provided by the Abstraction phase, but also able to mutate the templates and add or remove restrictions both automatically and guided by the user. Therefore, the creative acts are not restricted only to those that are known and should be able to surpass this stage as well.}
\item{\emph{Discovery stage}: the ability to work off templates derived from Analysis and Abstraction imply that the system is able to generate works that are sufficiently similar to be assessed with current contexts. However, given the flexibility of the mutation and user-guidance ability, it can also produce works that are significantly dissimilar. We believe the system to be able to reach this stage.}
\item{\emph{Disruption and Disorientation stages}: Since templates and constraints on the creative work that are imposed are the results of analysing and abstracting existing works, it is not the case that this system solely produces poetry that is too dissimilar to those known by theory. }
\end{itemize}

Therefore, we can see that this system should reach the desired \emph{Discovery stage} of the IDEA Descriptive Model. We will evaluate the results mathematically as per Colton, Charnley and Pease.

\subsection{Knowledge}
220,000 specks of knowledge vs 10,000 for Metaphor magnet, 9,000 for Perception.
Quality of sources is high quality, so knowledge is dependable.
Other sources include:
blah
blah
blah

So we are still limited on understanding of some words and could improve further.

Many specks use the most general \textit{RelatedTo} relation, which is only really useful for cohesiveness throughout the poem and do not help building the poems. Let's have a distribution...

Does it come up with new stuff each time? With the same input, how many words are reused in each poem? Is there too much weighting to the quality of the knowledge?

Google Suggestions is poor, should avoid using that

\section{Performance}

450 Limericks: 8040, 10 threads
154 Sonnets: 12004, one thread to avoid memory error
718 Haikus: 2808, 12 threads


\subsection{Concurrency and Performance of Interpretation}
\label{sec:interpret-perf}

Since poems are analysed and features are aggregated \textit{in isolation}, there is plenty of scope for concurrency.

For the poem analysis, a thread pool managed the concurrent execution of 10 workers, each analysing one poem. Trial and error showed 10 to be the optimal number of worker threads. Large resources such as the CMU Pronouncing Dictionary and WordNet were created once and shared amongst the threads.

Under this set up with machine specifications as given in the Appendix section \ref{sec:specs}, 450 limericks were analysed in an average of 2 hours and 14 minutes for the whole corpus.

This was sufficiently fast for this corpus and since this is data is pre-processed and stored. However, using Python's Pool Executor library rather than the Threading module enables us to move over to processes instead of threads with minimal effort if we need multi-core parallelism. It should be noted that this might be slower due to larger overhead and the need for large resources to be created in multiple locations since memory cannot be shared.

The generalisation process was actually fastest without the overhead of constructing the thread pool. With a thread pool of any number of workers, the average processing time is above 42 seconds for the results of the 450 analysed limericks. Without threading, the process took 40.1 seconds on average.

This generalisation process is fast enough for it to be done over again with different given values as discussed in Section \ref{sec:approach} earlier in the chapter. This is will be very important in getting precise guidance during the generalisation phase.

% ------------------------------------------------------------------------


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End: 
