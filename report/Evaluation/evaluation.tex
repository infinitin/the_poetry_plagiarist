\chapter{Evaluation}
\ifpdf
    \graphicspath{{Evaluation/EvaluationFigs/PNG/}{Evaluation/EvaluationFigs/PDF/}{Evaluation/EvaluationFigs/}}
\else
    \graphicspath{{Evaluation/EvaluationFigs/EPS/}{Evaluation/EvaluationFigs/}}
\fi

We evaluate this program by looking at the results of analysing 450 limericks, 718 haiku and all 154 of Shakespeare's Sonnets. These three types of poetry cover a large range of techniques and are of varying lengths so we can see changes in performance and quality with poem size.

\section{Comparison of Analysis Results to Literary Theory}
A major goal of this project is to show that the structure and content of types of poetry can be derived by analysing corpuses of collections. We would like to investigate how our results matches up with those documented in literary theory, as described by Young Writers[ref], a poetry and creative writing resource for UK schools, and poets.org[ref], supported by the Academy of American Poets.

\subsection{Limericks}

\begin{description}
\item[One stanza, five lines]  \hfill \\
All of the poems in our corpus follow this rule, as seen in Figure BLAH.


\item[Usually begins with \textit{'There was a...'} and ends with a name, person or place]  \hfill \\
Figure BLAH shows the N-Grams for line 1. We can see that \textit{'There be a'}, the lemmatised version of \textit{'There was a'} is used with significant frequency. We can also see in Figure BLAH that limericks largely talk about animate objects (such as people) or non-objects (such as places). Furthermore, Figure BLAH shows that the probability of a \textit{Named} or \textit{AtLocation} relation is higher in the first line than any other line.

Overall, our results supports the theory. However, since the analysis omits Named Entity Recognition, we are unable to specifically say whether or not it was a person or a place.

Furthermore, our results only show that x poems have this n-gram, which is only X\% of the corpus. We suspect this discrepancy comes from the popularity of Edward Lear poems, of which most start with \textit{'There was a...'}, but poems by other authors do not generally use this as a rule. This comes down to the contents of the corpus. Since it was a collection of previously unseen poems from various sources across the Internet, we do not know the proportion of Edward Lear poems in the corpus before analysis (as required to avoid biasing the corpus as mentioned in Section BLAH). If the poem had a higher proportion of Edward Lear limericks, the frequency of this property would probably increase. However, since we are analysing limericks in general, our result may be representative.

\item[Last line is farfetched or unusual]  \hfill \\
We have no way of determining this from our analysis or results because the social context of the poem was never considered. As a result this feature will only occur by chance, not intention.

\item[Usually nonsense or humour]  \hfill \\
As above.

\item[\textit{AABBA} rhyme scheme]  \hfill \\
Our results show that this is by far the most probable rhyme scheme, as seen in Figure BLAH. The other possibilities are close variations of \textit{AABBA}, eluding to the limitations of the CMU Pronunciation Dictionary and our method of handling words unknown to it, as described in Section BLAH. The results may yet improve if we were to build a state transducer model to predict the most likely rhythm pattern for words that are not in the dictionary.

Despite those shortcomings the results follow the ZipF law as discussed in Section BLAH, which means that we can treat the top result as unambiguous. Therefore, all limericks generated by the system will aim to have the \textit{AABBA} rhyme scheme.

\item[Lines 1, 2 and 5 have 01001001 meter, while lines 3 and 4 have 01001 meter]  \hfill \\
Figures BLAH through BLAH show the stress patterns of lines 1, 2 and 5.

The most popular rhyme scheme in line 1 is the expected one. However, it does not follow the ZipF law because the second option - the expected pattern with an appended unstressed syllable - is not significantly less probable.

The results for the second line are worse because the expected pattern is only third most probable. The others are variations on the expected pattern - first prefixed with a stressed syllable and then appended with an unstressed syllable. However, given the relative proportions of these three, it is not unlikely that the expected pattern will be selected.

The analysis of the fifth line is the worst. Not only is the expected pattern the third most popular, the graph follows the ZipF law, which means that the first, incorrect pattern (expected pattern prefixed with a stressed syllable) will be treated as unambiguous.

In all cases, the most probable patterns include the expected one or variations thereof. This is likely caused by the limitations of the CMU Pronunciation Dictionary again. We do not suspect it is our methodology because we try to accommodate for as many rhythm possibilities during analysis and we make no claims or filters along the way. 

Since our generation phase tries for a best fit rather than strictly the same pattern, this is not a damaging result in that respect.

Figures BLAH and BLAH show the stress patterns of lines 3 and 4. 

In both of these lines, the expected pattern is the third most popular and the most popular is the expected pattern prefixed with a stressed syllable. The most interesting thing is the relative proportions - both times the 101001 pattern would be treated as unambiguous. The reasoning for this comes down to three possibilities:
\begin{itemize}
\item{The limitations of the CMU Pronunciation Dictionary are greater than expected, but consistent. The syllable count is consistently one too high and happens to lead with the added stressed syllable very often as its mistake.}
\item{Our algorithms for detecting rhythm are flawed.}
\item{The corpus is very biased to this particular mistake.}
\item{Limericks actually have 101001 as the expected rhythm so our sources are wrong and perhaps this was never noticed.}
\end{itemize}

The first two are unlikely because the results for lines 1, 2 and 5 as well as for other poems (see following sections) are generally as expected. If either of those were in fact the problem, we would expect many more unexpected results.

The third is a possibility. The corpus of poems was collected from various sources across the Internet, but some websites provided more than others. It is likely that the same author who consistently made the same mistake was published on the same website that made up a large proportion of the corpus.

The fourth possibility requires further investigation into high quality limericks written by well known authors and more sources for the rhythm of limericks. Poets.org credits Edward Lear's \textit{Book of Nonsense}[ref] as the defining anthology for modern limericks.

The first 10 poems of that very book are shown in Appendix BLAH. If we analyse them by eye, the third line has six syllables 5 times and the fourth line has six syllables 8 times. From this we can assume that the expected rhythm of 01001 is actually not that common.

The Book of Limericks (Wordsword Reference) is a published anthology of over 1,800 limericks. This book was not used to build the corpus so it is unlikely that any of them would have been part of the original analysis. We select the first 10 from this analogy, as seen in Appendix \ref{sec:book}, and perform the same analysis by eye. In this case, the third line has six syllables 8 times and the fourth line has six syllables all 10 times.

This suggests that the standard is in fact to have six syllables, so the expected rhythm of 01001 is not correct. Therefore, we look to other sources of literary theory in case this has been pointed out.

http://www.poetry4kids.com/blog/lessons/how-to-write-a-limerick/ has 01001 and their examples use six syllables.
http://www.gigglepoetry.com/poetryclassdetail.aspx?LessonPlanID=2 claims the fourth line is 001001, but their example is five syllables.
http://www.creative-writing-now.com/how-to-write-a-limerick.html
http://www.poetryteachers.com/poetclass/lessons/limerick.html
http://freespace.virgin.net/merrick.sheldon/limerickrules.htm

Nowhere is it mentioned that the rhythm should be 101001. The five sources above teach the rhythm of limericks, but their teachings vary and the examples they use often do not match up with the theory that they give, but in most cases do match with the 101001 candidate.

We therefore, conclude that more investigation needs to be done to decide what is the \textit{actual} expected rhythm of the third and fourth line or limericks, and we further propose that based on our analysis it should be 101001.
\end{description}


\subsection{Haiku}

\begin{description}
\item[One stanza, three lines]  \hfill \\
\item[Topic is usually nature or similar]  \hfill \\
\item[5-7-5 syllabic rhythm]  \hfill \\
\item[No rhyme scheme]  \hfill \\
\item[No specific meter]  \hfill \\
\end{description}

\subsection{Shakespearean Sonnets}

\begin{description}
\item[One stanza, fourteen lines]  \hfill \\
\item[Topic is usually of thought, emotion and ideas]  \hfill \\
\item[\textit{ABABCDCDEFEFGG} rhyme scheme]  \hfill \\
\item[Iambic pentameter for every line]  \hfill \\
\end{description}

Large number of unknown words (can we get a number?). This makes it extremely slow because we find the closest word by spelling. As we know this does not always work for English, so this can be improved by using the state transducer model. Will also significantly speed up processing because we are not wasting time looking up the closest matching word, which is probably the slowest thing and why non-shakes ones go a lot faster.


\subsection{Summary}
Our results show that there is no need to hard code rules of poetry for most cases. Rhythm and rhyme tend to match up with theory despite the limitations of the CMUPD and the lack of quality in our corpuses.

Content understanding looks promising but still needs lots of work. There is currently no NE Recognition, a rather big omission but easily fixed.

Anaphora stuff:
This project does not delve deeply enough into this approach to fully test and evaluate it as it is outside the scope of this project. However, it is a new approach with potential that may be worth investigating further. 


\section{Turing-style Tests}
Can they fool a human?
Is it something that people can use?
Limericks: Blah
Sonnets: Blah
Haiku: Blah


\section{Quality of English}
Our approach to generation is \textbf{context first}, which requires a large vocabulary and control over grammar.
\subsection{Vocabulary and Word Selection}
The vocabulary of the system is dispersed among the third party and internal sources used.
WordNet: 155,000
CMUPD: 133,000
FrameNet: Blah
Knowledge Network: Blah
SimpleNLG Lexicon: Blah

Only x words are shared among all of these sources. Altogether, the number of unique words is y. The Oxford Enlgish Dictionary has z words so there is still much room for improvement. We have no scientific or colloquial vocabulary. 

The average vocabulary of native English speakers is 20,000-35,000[http://www.economist.com/blogs/johnson/2013/05/vocabulary-size], so in that sense we are very good. However, the amount of the vocabulary that is actually used is somewhat limited by the fact that we translate a relation (of which there are relatively few) to phrases. Selection from FrameNet is somewhat limited and not hugely dependable for many words. We use the general takes and receives action relations, but quality will significantly improve if we get more relations like desires, usedfor etc.

\subsection{Grammar}
This means that the quality of English should be rather high. We can check this by running the output through an over-the-counter grammar checker. We find that some percentage of lines do not fail the grammar checker, assuming a full stop at the end of every line.

FrameNet does not have all the answers.

Could take advantage of knowledge of length of syllables to determine the grammatical structure (e.g. intransitive verbs for short sentences).

Enjambent fails.

\section{Exhibition of Poetic Features}
Probabilistic. Rhyme and syllabic rythm are followed most of the time. However, by nature of the results being somewhat ambiguous, the results vary naturally.

Accentual rhythm is a best fit and is not as important as content. If we analyse the poem back using the analysis stage, we find that everything except this matches up with some permutation of the probability distribution, but not necessarily one specific instance. This is good because it means that we are getting \textit{close} to other previous poems, but not directly copying the grammatical structure like in that constraint programming one.

\section{Creativity}
Are the poems creative?
\subsection{FACE and IDEA Descriptive Models}
\subsubsection{FACE Descriptive Model}
\label{sec:face}
A full FACE model has four symptoms: examples, concepts, aesthetics and framing information.

\begin{itemize}
\setlength{\itemsep}{0pt}
\item{\emph{Examples} will be showcased by the templates generated by the Analysis and Abstraction phase.}
\item{\emph{Concepts} are of the form of the algorithm described for the Generation phase as it takes input from the user or online, the results from the Abstraction phase and several third party libraries to output a poem.}
\item{\emph{Aesthetics} are assessed by running the Analysis phase over the poem again. In fact, this happens several times during the creation of the poem. Any faults are reported back to the next iteration of that poem.}
\item{\emph{Framing Information} is the poem created.}
\end{itemize}

Therefore, we can see that this system should fully abide by the FACE Descriptive Model. We will evaluate the results mathematically as per Colton, Charnley and Pease.

\subsubsection{IDEA Descriptive Model}
\label{sec:idea}
A full IDEA model has six stages to which the software can reach. We want our software to be in the, fourth or \emph{Discovery stage}.

\begin{itemize}
\setlength{\itemsep}{0pt}
\item{\emph{Developmental stage}: this system has a full Abstraction phase to avoid the case that all creative acts undertaken by this system are purely based on inspiring examples. So this system will have surpassed this stage.}
\item{\emph{Fine-tuning stage}: the Abstraction phase only looks for a limited number of overlapping features to provide the template, leading to higher level abstraction. For example, it does not use part-of-speech tags from previous examples or any low level abstractions. We believe the system should be able to surpass this level.}
\item{\emph{Re-invention stage}: the system is able to work off a template provided by the Abstraction phase, but also able to mutate the templates and add or remove restrictions both automatically and guided by the user. Therefore, the creative acts are not restricted only to those that are known and should be able to surpass this stage as well.}
\item{\emph{Discovery stage}: the ability to work off templates derived from Analysis and Abstraction imply that the system is able to generate works that are sufficiently similar to be assessed with current contexts. However, given the flexibility of the mutation and user-guidance ability, it can also produce works that are significantly dissimilar. We believe the system to be able to reach this stage.}
\item{\emph{Disruption and Disorientation stages}: Since templates and constraints on the creative work that are imposed are the results of analysing and abstracting existing works, it is not the case that this system solely produces poetry that is too dissimilar to those known by theory. }
\end{itemize}

Therefore, we can see that this system should reach the desired \emph{Discovery stage} of the IDEA Descriptive Model. We will evaluate the results mathematically as per Colton, Charnley and Pease.

\subsection{Knowledge}
220,000 specks of knowledge vs 10,000 for Metaphor magnet, 9,000 for Perception.
Quality of sources is high quality, so knowledge is dependable.
Other sources include:
blah
blah
blah

So we are still limited on understanding of some words and could improve further.

Many specks use the most general \textit{RelatedTo} relation, which is only really useful for cohesiveness throughout the poem and do not help building the poems. Let's have a distribution...

Does it come up with new stuff each time? With the same input, how many words are reused in each poem? Is there too much weighting to the quality of the knowledge?

Google Suggestions is poor, should avoid using that

\section{Performance}

450 Limericks: 8040, 10 threads
154 Sonnets: 12004, one thread to avoid memory error
718 Haikus: 2808, 12 threads


\subsection{Concurrency and Performance of Interpretation}
\label{sec:interpret-perf}

Since poems are analysed and features are aggregated \textit{in isolation}, there is plenty of scope for concurrency.

For the poem analysis, a thread pool managed the concurrent execution of 10 workers, each analysing one poem. Trial and error showed 10 to be the optimal number of worker threads. Large resources such as the CMU Pronouncing Dictionary and WordNet were created once and shared amongst the threads.

Under this set up with machine specifications as given in the Appendix section \ref{sec:specs}, 450 limericks were analysed in an average of 2 hours and 14 minutes for the whole corpus.

This was sufficiently fast for this corpus and since this is data is pre-processed and stored. However, using Python's Pool Executor library rather than the Threading module enables us to move over to processes instead of threads with minimal effort if we need multi-core parallelism. It should be noted that this might be slower due to larger overhead and the need for large resources to be created in multiple locations since memory cannot be shared.

The generalisation process was actually fastest without the overhead of constructing the thread pool. With a thread pool of any number of workers, the average processing time is above 42 seconds for the results of the 450 analysed limericks. Without threading, the process took 40.1 seconds on average.

This generalisation process is fast enough for it to be done over again with different given values as discussed in Section \ref{sec:approach} earlier in the chapter. This is will be very important in getting precise guidance during the generalisation phase.

% ------------------------------------------------------------------------


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End: 
