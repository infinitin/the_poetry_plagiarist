\chapter{Analysis Interpretation}
\ifpdf
    \graphicspath{{Implementation/ImplementationFigs/PNG/}{Implementation/ImplementationFigs/PDF/}{Implementation/ImplementationFigs/}}
\else
    \graphicspath{{Implementation/ImplementationFigs/EPS/}{Implementation/ImplementationFigs/}}
\fi

The second phase of implementation involves interpreting the results of the analysis done in the first phase. The goal is that these interpretations can be used to template the structure and outline the content for certain collections of poems, ultimately guiding the generation of novel poetry in the final phase.

\section{Priorities}
In deciding how to approach this phase, we need to make some predictions about how we are going to use the final interpretation and assumptions about the quality, or lack thereof, of the corpus. Doing this will outline the priorities and trade-offs that can then be implemented in our approach.


\subsection{The Corpus}

The corpus is a large collection of poems. We may assume that each poem is already marked with the type of the collection - limerick, haiku, riddle etc. - in lieu of data in a \textit{supervised} machine learning problem. However, the poems should otherwise be unseen before analysis; we know nothing else about the data. 

Poems are most easily obtained from anthologies - books of poems usually of a certain type. This works very well with our assumptions; the data is already labelled with the type of poem by looking at the type of anthology, but we know nothing else about the contents.

Another way to gather these poems is by looking for those freely available on the Internet. However, the corpus obtained from an Internet search will most likely be lower quality than directly from a published anthology. For example, we may get the limerick in Figure \ref{fig:awkward-limerick} that is clearly very awkward. We can immediately see that it does not follow the typical \textit{'AABBA'} rhyme scheme, and even more so that it doesn't have a fifth line!

\begin{figure}[h!]
\centering
\textit{
A limerick fan from Australia\\
regarded his work as a failure:\\
his verses were fine\\
until the fourth line\\
?
}
\caption{A rhyming quatrain often used in teaching poetry}
\label{fig:awkward-limerick}
\end{figure}
Nevertheless, our blindness to the content of the corpus means the system must be robust to such outliers. This means that we \textit{will} get anomalous results in our data that should be ignored.

\subsection{Subcategories of Collections}

The first two stages aim to be an exhaustive analysis of any type of poem. The first stage achieves this by detecting as many features in a single poem as possible. The second stage does this not only by finding highlights among features of homogeneous poems, but correlations between them as well.

For example, limericks that start with \textit{'There was a...'} may have a very distinctive rhythm pattern for the first line that is otherwise not strictly followed. Therefore, when generating new poetry in the next phase that starts with \textit{'There was a...'}, we want to make sure we abide by this correlation.

Naturally, there will be many correlations with more than two variables that may be more difficult to spot, but the problem of finding all of these correlations is exponential. We need to find a balance between using this data and the complexity in obtaining it.

\subsection{Novel and Creative Generation}

There's another catch to the correlation problem - red herrings. Some features can be taken as law; 5 lines in a limerick, 5-7-5 syllable pattern in Haiku, iambic pentameter in Shakespearean Sonnets. However, some may just be coincidences or a very biased corpus.

Furthermore, we want to be able to create poems that are \textit{novel} and \textit{creative}. So while we need to be guided by prior art on structure and content to create authentic pieces of literary art, we need to balance that with freedom of creativity. Perhaps this generalisation stage will also give us an idea of when and how we are allowed to go off and explore new routes.


\section{A Probabilistic Approach}

The most suitable implementation that meets the priorities listed above is:
\begin{itemize}
\item{Analyse each poem individually as planned.}
\item{Store collections of poems by type (limerick, haiku etc.). This avoids unnecessary filtering downstream. This stored data can also be made open source to be used for future research.}
\item{Put the analysis results of each poem in a collection together, keeping each feature independent. For example, we put all of the rhyme schemes for all poems of a collection in one big list, saving it separately from a similar list of stanza lengths. I will refer to this process as \textit{'feature aggregation'} from now on.}
\item{Then for each feature, create a distribution of the results that represents the probability of any particular value occurring for each feature. If 95 out of a 100 limericks have an AABBA rhyme scheme, then we consider the probability of an AABBA rhyme scheme in a limerick to be 95\%.}
\end{itemize}

The aggregation of any feature can be bespoke to that feature. For example, we would aggregate the tenses for each line of each poem differently to the way we would aggregate the overall tense for each poem. This enables us to handle outliers on a case-by-case basis since what is considered an 'outlier' will vary feature-by-feature.

We can also decide how to interpret the data on a case-by-case basis. This helps us handle the varying error bands for each feature. For example, since detecting the rhyme scheme is more error-prone than detecting the number of syllables, we may round a 95\% result for rhyme scheme up to 100\%, but take it literally for syllable pattern.

This implementation also gives a simple way of finding correlations. We simply take a value for one of the features as a \textit{'given'}, filter the store of analysed poems by this value and aggregate in the same way as before. We can take any number of givens and of any permutation since each feature is aggregated and interpreted in isolation.

The added bonus to all of this is that it is quite efficient both in terms of time and space. By storing the original poem analysis results, we do not need to find correlations until they are needed, which we can then do by applying the appropriate given.


\section{Algorithms}

There are four main algorithms that are used to aggregate features. All features can be aggregated using some variant of these algorithms, with slight feature-specific amendments so to have the flexibility and precision mentioned in the previous sections.

\subsection{List Comprehension}
\label{sec:listcomp}

The simplest of the four algorithms utilises Python's built in functional programming feature; list comprehensions. In a single line of code we put all of the results for a particular feature from all poems in the collections into one big list. Algorithm BLAH shows the code for aggregating the number of stanzas.

\textit{
1. num\_stanzas = [poem.n\_stanzas for poem in poems]\\
2. return Count(num\_stanzas)
}

The second line gets the frequency distribution of this list, which can easily be turned into probabilities as required.


\subsection{Line by Line}

Sometimes we can gain more insight by interpreting patterns in a feature for \textbf{each line} rather than the poem altogether. In these cases, the analysis result is usually saved in the form of a tuple, one entry for each line.

Take stress patterns for example. The analysis of a single poem will store the result as \texttt{(x$_1$, x$_2$, ..., x$_n$)} where \texttt{x$_i$} is the stress pattern for line \texttt{i} up to the total number of lines in that poem, \texttt{n}.

Suppose we have three poems in the corpus altogether. Then we have three tuples to aggregate: \texttt{(x$_1$, x$_2$, ..., x$_n$)}, \texttt{(y$_1$, y$_2$, ..., y$_n$)} and \texttt{(z$_1$, z$_2$, ..., z$_n$)}. If we are looking generalise the results on a line by line basis, we want to \textbf{transpose} the data to group each line together and put them in a list with the format \texttt{[(x$_1$, y$_1$, z$_1$), (x$_2$, y$_2$, z$_2$), ... ,(x$_n$, y$_n$, z$_n$)]}.

Once this is done, we can apply the earlier list comprehension algorithm described in Section \ref{sec:listcomp}. Algorithm BLAH outlines this full process with the stress pattern example.

\textit{
1. stress\_patterns = [poem.stress\_patterns for poem in poems]\\
2. stress\_patterns = transpose(stress\_patterns)\\
3. return [Count(stress\_pattern\_single\_line) for stress\_pattern\_single\_line in stress\_patterns]\\
}

\subsection{Pick the Most Popular}

The analysis of a single poem often has some ambiguities. For example, the rhyme scheme for a collection can never be decided by looking at just one poem because words can be pronounced in more than one way. The limerick in Figure BLAH could either be 'AABBA' or 'ABCCA' because \textit{blah} can be pronounced as \textit{('BL', 'AH')} or \textit{('BL', 'EH')}.

Because of this, the analysis phase doesn't make a claim on which rhyme scheme is the correct one and instead stores all possibilities in a list \texttt{[x$_1$, x$_2$, ..., x$_n$]}, where \texttt{x$_i$} is the \texttt{i\textsuperscript{th}} rhyme scheme possibility up to \texttt{n} possibilities for the particular poem being analysed.

Let's assume again that we only have three poems in the corpus. Once they are all analysed, we get three lists \texttt{[x$_1$, x$_2$, ..., x$_n$]}, \texttt{[y$_1$, y$_2$, ..., y$_n$]} and \texttt{[z$_1$, z$_2$, ..., z$_n$]} of possible rhyme schemes for all three poems.

The next step can be done inductively, but I will illustrate what we want to happen by taking three cases:
\begin{itemize}
\item{None of the possibilities across all poems are the same. Then all are equally probable and that gives us our probability distribution.}
\item{There is \textit{one} set \texttt{(p, q, r)} such that \texttt{x$_p$} == \texttt{y$_q$} == \texttt{z$_r$}. Then we take that to be the rhyme scheme and no other possibility is considered.}
\item{There is more than one set \texttt{(p$_1$, q$_1$, r$_1$), (p$_2$, q$_2$, r$_2$), ..., (p$_m$, q$_m$, r$_m$)} such that for all \texttt{i} up to \texttt{m}, \texttt{x$_p{_i}$} == \texttt{y$_q{_i}$} == \texttt{z$_r{_i}$}. Then all sets are candidate rhyme schemes where each rhyme scheme has a probability of \texttt{1/m}.}
\item{There is a \texttt{p} and a \texttt{q} such that \texttt{x$_p$} == \texttt{y$_q$}, but no \texttt{r} such that \texttt{y$_q$} == \texttt{z$_r$}. In this case, the rhyme scheme \texttt{x$_p$} has a probability of two thirds, while every rhyme scheme in z has the probability of one third each.}
\end{itemize}

Essentially we want to find the most popular rhyme scheme among all poems, then remove all sets of possibilities that have it. We then repeat on the remaining poems over and over until we have no more poems. 

Algorithm BLAH shows the pseudo-code for this implementation.

\textit{
 1. rhyme\_scheme\_counts = {}\\
 2. possible\_rhyme\_scheme\_lists = [poem.rhyme\_schemes for poem in poems]\\
 3. all\_possible\_rhyme\_schemes = flatten(possible\_rhyme\_schemes)\\
 4. most\_popular\_rhyme\_schemes = most\_common(all\_possible\_rhyme\_schemes)\\
 5. for each rhyme\_scheme in most\_popular\_rhyme\_schemes\\
 6. 		rhyme\_scheme\_counts[most\_popular\_rhyme\_scheme)] = all\_possible\_rhyme\_schemes.count(rhyme\_scheme)\\
 7. for each possible\_rhyme\_scheme\_list in possible\_rhyme\_scheme\_lists\\
 8.		if possible\_rhyme\_scheme\_list contains any of  most\_popular\_rhyme\_schemes\\
 9.			remove possible\_rhyme\_scheme\_list from possible\_rhyme\_scheme\_lists\\
10. if possible\_rhyme\_scheme\_lists is not empty, go to 3.\\
11. return rhyme\_scheme\_counts\\
}


\subsection{Filter Below Threshold}

This algorithm deals mostly with content-based features such as n-grams, persona relations and types of persona. These features are varied, unpredictable, error-prone and highly sensitive to bias in the corpus. However, they can still provide very interesting and useful data to guide generation and create authentic poems. For example, starting a limerick with 'There was a...' and talking about nature in Haikus.

In this case, we perform any of the three aforementioned algorithms to collect all possible results. We then apply a filter so that only results that occur with \textit{significant frequency} can be taken into account.

Unfortunately, there is no way to tell what frequency can really be counted as significant. It will vary greatly between types and subcategories of poems and is prone to many red-herrings. At the moment, we arbitrarily set the threshold to 10\% of the corpus, but this will need further research to refine.

\section{Results}
- Pretty graphs!
- Some interpretations
- How it can be used in next stage
- How it can be extended upon

\section{Performance}
- Threaded analysis
- 2 hours 7 minutes
- No parallelism, but space for it
- 40 seconds without graphs, 75 extra seconds with



% ------------------------------------------------------------------------

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End: 
