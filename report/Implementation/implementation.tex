\chapter{Analysis Interpretation}
\label{chp:interpretation}
\ifpdf
    \graphicspath{{Implementation/ImplementationFigs/PNG/}{Implementation/ImplementationFigs/PDF/}{Implementation/ImplementationFigs/}}
\else
    \graphicspath{{Implementation/ImplementationFigs/EPS/}{Implementation/ImplementationFigs/}}
\fi

The second phase of implementation involves interpreting the results of the analysis phase. The goal is that these interpretations can be used to template the structure and outline the content for certain collections of poems, ultimately guiding the generation of novel and authentic poetry in the final phase.

\section{Approach}
\label{sec:balance-priorities}

In deciding how to approach this phase, we need to make predictions about the usage of the final interpretation and assumptions about the quality, or lack thereof, of the corpus and analysis.


\subsection{The Corpus}
\label{sec:corpus}

The corpus is a large collection of poems. We may assume that each poem is already marked with the \textit{type} of the collection - limerick, haiku, riddle etc. - much like data in a supervised machine learning problem. However, the poems should otherwise be \textit{unseen} before analysis; we know nothing else about them. 

Poems are most easily obtained from anthologies. This works very well with our assumptions as the data is already labelled with the type of poem, but we know nothing else about the content.

Another way to gather poems is by looking for those freely available on the Internet. However, the corpus obtained from an Internet search will likely be of lower quality than directly from a published anthology. For example, we may get the limerick in Figure \ref{fig:awkward-limerick}, which is clearly very awkward. We can immediately see that it does not follow the typical \textit{AABBA} rhyme scheme, and even more so that it doesn't have a fifth line!

\begin{figure}[h!]
\centering
\textit{
A limerick fan from Australia\\
regarded his work as a failure:\\
his verses were fine\\
until the fourth line\\
?
}
\caption{A very awkward limerick for analysis}
\label{fig:awkward-limerick}
\end{figure}
Nevertheless, our blindness to the content of the corpus means the system must be robust to such outliers. Therefore, we \textit{will} get anomalous results and need to be able to cope with them.

\subsection{Subcategories of Collections}

The first two stages aim to be an exhaustive analysis of any type of poem. The first stage achieves this by detecting as many features in a single poem as possible. The second stage does this not only by finding highlights among features of homogeneous poems, but correlations between them as well.

For example, limericks that start with \textit{'There was a...'} may have a very distinctive rhythm pattern for the first line that is otherwise not strictly followed. Therefore, when generating new poetry in the next phase that starts with \textit{'There was a...'}, we want to make sure we abide by this correlation.

There will be many correlations with more than two variables that may be more difficult to spot, but the complexity of finding all of them is exponential. We need to find a balance between the usefulness of this data and the complexity in obtaining it.

\subsection{Novel and Creative Generation}

There is another catch to the correlation problem - red herrings. Some features can be taken as law; 5 lines in a limerick, 5-7-5 syllable pattern in Haiku, iambic pentameter in Shakespearean Sonnets. However, some may be coincidences or the result of a very biased corpus.

Furthermore, we want to be able to generate poems that are \textit{novel} and \textit{creative}. So while we need to be guided by prior art on structure and content to create authentic pieces of literary art, we need to balance that with freedom of creativity. 


\section{A Stochastic Approach}
\label{sec:approach}

The most suitable implementation that meets the priorities listed above is:
\begin{itemize}
\item{Analyse each poem individually as planned.}
\item{Store collections of poems by type (limerick, haiku etc.). This avoids extra filtering and repeated memory persistence downstream.}
\item{Put the analysis results of each poem in a particular collection together, keeping each feature independent. For example, we put all of the rhyme schemes for all limericks in one big list, saving it separately from the list of stanza lengths. I will refer to this step as \textit{'aggregation'} from now on.}
\item{For each feature, create a probability distribution of the results that represents the likelihood of any particular value occurring for each feature. If 95 out of 100 limericks have an \textit{AABBA} rhyme scheme, then we consider the probability of an \textit{AABBA} rhyme scheme in a limerick to be 95\%.}
\end{itemize}

The aggregation step is bespoke to each feature. For example, we aggregate the tenses for each line of each poem differently to the way we would aggregate the overall tense for each poem. This enables us to handle outliers on a case-by-case basis since they are defined differently for every feature.

We can also decide how to \textit{interpret} the data on a case-by-case basis, helping us handle the varying error bands for each feature. For example, detecting the rhyme scheme is more error-prone than detecting the number of syllables so we may round a 95\% result for rhyme scheme up to 100\%, but take it literally for syllable pattern.

This implementation also gives an effective way of finding correlations. We take a value for one of the features as a \textit{given}, filter the store of analysed poems by this value and aggregate. We can take any number of given values and of any permutation since each feature is aggregated and interpreted in isolation.

As a bonus, this implementation quite efficient both in terms of time and space (see Section \ref{sec:interpret-perf}). By storing the original poem analysis results, we can apply given values and find correlations \textit{lazily}, i.e. only when needed.


\section{Algorithms}

All features can be aggregated using some variant of one of four algorithms, with slight feature-specific amendments so to have the flexibility and precision mentioned in the previous sections.

\subsection{List Comprehension}
\label{sec:listcomp}

The simplest of the four algorithms utilises Python's built in functional programming feature; list comprehensions. In a single line of code we put all of the results for a particular feature from all poems in the collection into one big list. The following algorithm shows the code for aggregating the number of stanzas.

\begin{verbatim}
1. num_stanzas = [poem.n_stanzas for poem in poems]
2. return Count(num_stanzas)
\end{verbatim}

The second line gets the frequency distribution of this list, which can easily be turned into a probability distribution as required.


\subsection{Line by Line}

Sometimes we can gain more insight by interpreting patterns in a feature for \textbf{each line} rather than the poem altogether. In these cases, the analysis result is usually saved in the form of a tuple, one entry for each line.

Take stress patterns for example. The analysis of a single poem will store the result as \texttt{(x$_1$, x$_2$, ..., x$_n$)} where \texttt{x$_i$} is the stress pattern for line \texttt{i} up to the total number of lines in that poem, \texttt{n}.

Suppose we have three poems in the corpus altogether. Then we have three tuples to aggregate: \texttt{(x$_1$, x$_2$, ..., x$_n$)}, \texttt{(y$_1$, y$_2$, ..., y$_n$)} and \texttt{(z$_1$, z$_2$, ..., z$_n$)}. 

If we are looking generalise the results on a line by line basis, we want to \textbf{transpose} the data to group each line together and put them in a list with the format \texttt{[(x$_1$, y$_1$, z$_1$), (x$_2$, y$_2$, z$_2$), ... ,(x$_n$, y$_n$, z$_n$)]}.

Once this is done, we can apply the list comprehension algorithm described in Section \ref{sec:listcomp} to each tuple in the list. The following algorithm outlines this full process with the stress pattern example.

\begin{verbatim}
1. stress_patterns = [poem.stress_patterns for poem in poems]
2. stress_patterns_all_lines = transpose(stress_patterns)
3. return [Count(stress_pattern_single_line) for stress_pattern_single_line in stress_patterns_all_lines]
}
\end{verbatim}

\subsection{Pick the Most Popular}

The analysis of a single poem often has some ambiguities. For example, the rhyme scheme for a collection can never be decided by looking at just one poem because words can be pronounced in more than one way. The limerick in Figure \ref{fig:ambiguous-rhyme} could either be \textit{AABBA} or \textit{AABBC} because \textit{invited} can be pronounced as \textit{[IH N V AY T \textbf{AH} D]} or \textit{[IH0 N V AY T \textbf{IH}, D]} according to the CMU pronunciation dictionary.

\begin{figure}[h!]
\centering
\textit{
There was a young man so benighted\\
He never knew when he was slighted;\\
He would go to a party\\
And eat just as hearty,\\
As if he'd been really invited.\\
}
\caption{A limerick with more than one possible rhyme scheme}
\label{fig:ambiguous-rhyme}
\end{figure}

The analysis phase does not make a claim on which rhyme scheme is accepted and instead stores all possibilities in a list \texttt{[x$_1$, x$_2$, ..., x$_n$]}, where \texttt{x$_i$} is the $i$\textsuperscript{th} rhyme scheme possibility up to \texttt{n} possibilities for the particular poem being analysed.

Let's assume again that we only have three poems in the corpus. Once they are all analysed, we get three lists \texttt{[x$_1$, x$_2$, ..., x$_n$]}, \texttt{[y$_1$, y$_2$, ..., y$_n$]} and \texttt{[z$_1$, z$_2$, ..., z$_n$]} of possible rhyme schemes for each poem.

We illustrate what we want to happen by taking four cases:
\begin{itemize}
\item{All possibilities across all poems are unique. Therefore, all are equally probable and that gives us our probability distribution.}
\item{There is \textit{one} set of indices $(p, q, r)$ such that \texttt{x$_p$} == \texttt{y$_q$} == \texttt{z$_r$}. Then we take that to be the rhyme scheme and no other possibility is considered.}
\item{There is $m$\textit{\textgreater 1} sets of indices $(p_1, q_1, r_1), (p_2, q_2, r_2), ..., (p_m, q_m, r_m)$ such that for all $i$ up to $m$, \texttt{x$_{p_i}$} == \texttt{y$_{q_i}$} == \texttt{z$_{r_i}$}. Then all \textbf{sets} are candidate rhyme schemes where each rhyme scheme has a probability of $1/m$.}
\item{There is a $p$ and a $q$ such that \texttt{x$_p$} == \texttt{y$_q$}, but no $r$ such that \texttt{y$_q$} == \texttt{z$_r$}. In this case, the rhyme scheme \texttt{x$_p$} has a probability of two thirds, while every rhyme scheme \texttt{z$_i$} has the probability of one third each.}
\end{itemize}

An iterative implementation makes this process simple. We first find the most popular rhyme scheme among all poems in a set and save its frequency. We then remove all poems that contain the most popular rhyme scheme from the set and repeat on the remaining poems until there are no more poems in the set. 

The pseudo-code for this implementation is:

\begin{verbatim}
 1. rhyme_scheme_counts = {}
 2. possible_rhyme_scheme_lists = [poem.rhyme_schemes for poem in poems]
 3. all_possible_rhyme_schemes = flatten(possible_rhyme_schemes)	
 4. most_popular_rhyme_schemes = most_common(all_possible_rhyme_schemes)
 5. for each rhyme_scheme in most_popular_rhyme_schemes
 6.   rhyme_scheme_counts[most_popular_rhyme_scheme)] = all_possible_rhyme_schemes.count(rhyme_scheme)
 7. for each possible_rhyme_scheme_list in possible_rhyme_scheme_lists
 8.	  if possible_rhyme_scheme_list contains any of  most_popular_rhyme_schemes
 9.		  remove possible_rhyme_scheme_list from possible_rhyme_scheme_lists
10. if possible_rhyme_scheme_lists is not empty, go to 3.
11. return rhyme_scheme_counts
\end{verbatim}

\subsection{Filter Below Threshold}

This algorithm deals mostly with content-based features such as n-grams, persona relations and types of persona. These features are varied, unpredictable, error-prone and highly sensitive to bias in the corpus. However, they can still provide very interesting and useful data to guide generation and create authentic poems. For example, starting a limerick with \textit{'There was a...'} and talking about nature in Haikus.

In this case, we perform the most suitable of the three aforementioned algorithms to collect all possible results. We then apply a filter so that only results that occur with \textit{significant frequency} can be taken into account.

Unfortunately, there is no way to tell what frequency can really be counted as significant. It will vary greatly between types and subcategories of poems and is prone to many red-herrings. Trial and error has given the optimal threshold of 9\% of the corpus, but this will need further research to refine.

\section{Testing and Validation by Inspection}

We test and validate this phase by looking at two factors:
\begin{enumerate}
\item{Accuracy of the assumptions and predictions made at the start of this chapter.}
\item{Comparison of results with known poetry theory.}
\end{enumerate}

While some automated tests can be written to make sure that the algorithms above are implemented correctly, there is no desired output against which to compare. We cannot check for exact matches with poetry theory because this is an \textit{investigation} where known theory is the hypothesis. Writing tests that fail if the hypothesis is not correct is an investigative fallacy.

Therefore, the best way to test and validate the results of this stage is by inspection. Python provides a convenient interface for graphing large sets of data, which also make it easy to inspect by eye. 

The corpus used to test the system comprises of 450 limericks scrounged from the Internet. As mentioned earlier, poems gathered from the Internet are likely to have more outliers and anomalies, which makes for a more rigorous examination of the quality of our analysis and interpretation. 

A noteworthy subset of the results are shown in Figures \ref{fig:rhyme-scheme-chart} through \ref{fig:overall-tense-chart}. The y-axis is given in raw values instead of probabilities for this testing stage.
\newcommand{\exedout}{%
  \rule{0.8\textwidth}{0.5\textwidth}%
}

\begin{figure}[t!]
\centering
\includegraphics[height=.4\textheight]{StanzaLines}
\caption{Five lines long, one stanza}
\label{fig:stanza-lines-chart}
\end{figure}

\begin{figure}[t!]
\centering
\includegraphics[height=.4\textheight]{RhymeSchemes}
\caption{AABBA rhyme scheme by far the most common}
\label{fig:rhyme-scheme-chart}
\end{figure}

\begin{figure*}[t!]
    \centering
    \begin{subfigure}[t]{0.9\textwidth}
        \centering
        \includegraphics[height=.4\textheight]{PersonaTypes}
        \caption{Limericks talk a lot about people}
    \end{subfigure}%
    
    \begin{subfigure}[t]{0.9\textwidth}
        \centering
        \includegraphics[height=.4\textheight]{PersonaAnimation}
        \caption{Most persona are either animate objects or 'concepts' such as places}
    \end{subfigure}
    \caption{Limericks generally talk about people and other animate objects}
    \label{fig:persona-chart}
\end{figure*}

\begin{figure}[t!]
\centering
\includegraphics[height=.4\textheight]{PersonaRelation}
\caption{Limericks talk about who these people are, what properties they have, what they are named, where they are, their capabilities, what they do, what is done to them etc.}
\label{fig:persona-relation-chart}
\end{figure}

\begin{figure}[t!]
\centering
\includegraphics[height=.4\textheight]{Line1NGrams}
\caption{The first line often introduces the person, usually as either old or young}
\label{fig:n-grams-1-chart}
\end{figure}

\begin{figure}[t!]
\centering
\includegraphics[height=.4\textheight]{DistinctSentences}
\caption{Whole poem seems to be either one or two sentences. This actually tells us that full stops are not needed, but if they are used then the whole poem is generally two sentences long.}
\label{fig:distinct-sentences-chart}
\end{figure}

\begin{figure}[t!]
\centering
\includegraphics[height=.4\textheight]{OverallTense}
\caption{Never in future tense, mostly in past an sometimes in present}
\label{fig:overall-tense-chart}
\end{figure}

The results of these and other corpora will be explored fully in the Evaluation. It is sufficient for the moment to say that these results are usable in the generation phase to apply rules but also guides us on the strength of these rules, suggesting where divergence may be applicable.


% ------------------------------------------------------------------------

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End: 
